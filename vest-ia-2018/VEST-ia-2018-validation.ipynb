{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting maps\n",
    "import maup # mggg's library for proration, see documentation here: https://github.com/mggg/maup\n",
    "import pandas as pd # standard python data library\n",
    "import geopandas as gp # the geo-version of pandas\n",
    "import numpy as np \n",
    "from statistics import mean, median\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import wrap\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 150)\n",
    "pd.set_option(\"display.max_columns\", 150)\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "DATA_PATH = \"raw-from-source/\"\n",
    "CRS = 3857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General Summary**\n",
    "\n",
    "-------\n",
    "\n",
    "#### VEST Data\n",
    "\n",
    "VESTs dataset uses IA's 2018 Election Results and Precinct Boundaries for it's shapefile. \n",
    "\n",
    "VEST data files include:  \n",
    "\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  **ia_2018.shp** which has both election results and shapefiles, can be found at [this link](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/UBKYRU/CVWIH1). \n",
    "\n",
    "\n",
    "\n",
    "The following counties used shapefiles sourced from the respective county governments instead: Dallas, Johnson, Linn, Story. \n",
    "\n",
    "The following precincts were merged to match the 2018 election reports:\n",
    "\n",
    "1. Appanoose: Udell/Union\n",
    "2. Black Hawk: Cedar Falls W2P2/Cedar Falls Twp\n",
    "3. Des Moines: Burlington 1/Tama, Burlington 8/Concordia\n",
    "4. Fremont: Hamburg/Washington, Farragut/Shenandoah1\n",
    "5. Lee: Fort Madison 4A/4B, Keokuk 2A/2B, FCM/Harrison\n",
    "6. Polk: Grimes 2/Urbandale 12\n",
    "7. Tama: Toledo 1/2/3\n",
    "\n",
    "The election data columns reported are:\n",
    "\n",
    "<font color=\"Coral\">\n",
    "    \n",
    "\n",
    "Secretary of State\n",
    "    \n",
    "    1. G18SOSRPAT - Paul D. Pate (Republican Party)\n",
    "    2. G18SOSDDEJ - Deidre DeJear (Democratic Party)\n",
    "    3. G18SOSLOFE - Jules Ofenbakh (Libertarian Party)\n",
    "    4. G18SOSOWRI - Write-in Votes\n",
    "    \n",
    "Governor\n",
    "    \n",
    "    1. G18GOVRREY - Kim Reynolds (Republican Party)   \n",
    "    2. G18GOVDHUB - Fred Hubbell (Democratic Party)\n",
    "    3. G18GOVLPOR - Jake Porter (Libertarian Party)\n",
    "    4. G18GOVOSIE - Gary Siegwarth (Clear Water Party)\n",
    "    5. G18GOVOWRI - Write-in Votes\n",
    "    \n",
    "Attorney General\n",
    "    \n",
    "    1. G18ATGDMIL - Tom Miller (Democratic Party)\n",
    "    2. G18ATGLBAT - Marco Battaglia (Libertarian Party)\n",
    "    3. G18ATGOWRI - Write-in Votes\n",
    "\n",
    "Treasurer\n",
    "    \n",
    "    1. G18TRERDAV - Jeremy N. Davis (Republican Party)\n",
    "    2. G18TREDFIT - Michael L. Fitzgerald (Democratic Party)\n",
    "    3. G18TRELHIR - Timothy Hird (Libertarian Party)\n",
    "    4. G18TREOWRI - Write-in Votes\n",
    "\n",
    "    \n",
    "Auditor\n",
    "    \n",
    "    1. G18AUDRMOS - Mary Mosiman (Republican Party)\n",
    "    2. G18AUDDSAN - Rob Sand (Democratic Party)\n",
    "    3. G18AUDLPER - Fred Perryman (Libertarian Party)\n",
    "    4. G18AUDOWRI - Write-in Votes\n",
    "    \n",
    "\n",
    "Agriculture Secretary\n",
    "    \n",
    "    1. G18AGRRNAI - Mike Naig (Republican Party)\n",
    "    2. G18AGRDGAN - Tim Gannon (Democratic Party)\n",
    "    3. G18AGRLSTE - Rick Stewart (Libertarian Party)\n",
    "    4. G18AGROWRI - Write-in Votes\n",
    "\n",
    "</font>\n",
    " \n",
    "\n",
    "----------------\n",
    "\n",
    "#### Raw shapefile data\n",
    "\n",
    "VEST reportedly collected shapefile data from the [IA Secretary of State](https://www.sos.IA.gov/elections/research/election-results-and-voters-pamphlets.aspx). \n",
    "\n",
    "SoS shapefile data for IA found [here](https://sos.iowa.gov/shapefiles/Statewide%20Precinct%20Layer/). \n",
    "\n",
    "The IA Secretary of State's data files include: \n",
    "\n",
    "   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  **Precincts041714.sbx** which contains shapefiles for precincts and counties that mostly align with the election results data.\n",
    "\n",
    "County shapefiles for [Dallas](http://geodallas.dallascountyiowa.gov/SpatialDownload/Default.aspx), [Johnson](https://www.iowagisdata.org/index.php/apps/files/?dir=/County/Johnson/Open&fileid=854), and [Linn](https://opendata-linncounty-gis.opendata.arcgis.com/datasets/voting-precinct-split-1?geometry=-92.535%2C41.808%2C-90.777%2C42.165) county were taken directly from county websites, in accordance with VEST's process. Data not posted on county websites was generally found on [Iowa's GIS Hub](https://www.iowagisdata.org/). \n",
    "\n",
    "For Story county, [GIS coordinator](https://www.storycountyiowa.gov/Directory.aspx?did=31) Matt Boeck was contacted directly and provided precinct level data via email. \n",
    "\n",
    "The following counties were revised to reflect updated municipal boundaries using shapefiles from the U.S. Census Bureau's 2020 Redistricting Data Program Phase 2 release: Dubuque, Marion, Muscatine, Polk, Pottawattamie, Scott.\n",
    "\n",
    "These counties were downloaded using this [link](https://www.census.gov/geo/partnerships/pvs/partnership19v2/st19_ia.html).\n",
    "\n",
    "-------\n",
    "\n",
    "#### Raw election results data\n",
    "\n",
    "Most election results were pulled from [this statewide election data site](https://results.vote.IA.gov/), county by county. The data was stored in a deprecated .xls format that needed to be processed and joined semi-manually. The work for this can be found in  **VEST-ia-2018-data-accumulation.ipynb**, present in this repo.\n",
    "   \n",
    "\n",
    "-----\n",
    "\n",
    "All of the data IAs accessed and downloaded between the dates of March 23rd and March 30th, 2021. \n",
    "\n",
    "\n",
    "#### <font color=\"red\">Attention:</font>\n",
    "RDH was not able to validate the precincts in all counties, specifically, Linn, Dubuque, Story, and Polk county had too many unregistered modifications for us to edit. VEST's precinct shapefiles for those counties could very well be correct, but we had no way of verifying. We can verify that the vote totals for these counties' precincts, and all precincts for that matter, are correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload, modify, and analyze bulk of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_vest_df = gp.read_file(DATA_PATH + \"vest_data/ia_2018.shp\")\n",
    "master_sos_df = pd.read_csv(DATA_PATH + \"SoS_processed_election_final.csv\")\n",
    "master_shape_df = gp.read_file(DATA_PATH + \"SoS_shapefiles/Precincts041714.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vest_df = master_vest_df.copy().to_crs(CRS)\n",
    "sos_df = master_sos_df.copy()\n",
    "shape_df = master_shape_df.copy().to_crs(CRS)\n",
    "\n",
    "sos_df[\"County\"] = sos_df[\"County\"].str.replace(\"_\", \" \")\n",
    "sos_df[\"County\"] = sos_df[\"County\"].str.replace(\"'\", \"\")\n",
    "\n",
    "obrien_fix = {\"OBrien\" : \"Obrien\"}\n",
    "def fix_obrien(val):\n",
    "    if val in obrien_fix:\n",
    "        return obrien_fix[val]\n",
    "    return val\n",
    "\n",
    "sos_df[\"County\"] = sos_df[\"County\"].apply(fix_obrien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vest_df.plot(figsize=(8,8))\n",
    "shape_df.plot(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precinct names within each county are quite different between the raw SoS election data and both VEST and shapefile data. \n",
    "\n",
    "##### Addidionally, precinct names are not unique in the SoS or VEST files, so we will need to create a unique ID by combining county and precinct name. \n",
    "\n",
    "##### So, first we will validate vote counts of each county, and using those results, will construct a dataframe that maps SoS precinct names to VEST precinct names. \n",
    "\n",
    "##### The general vote validation process is:\n",
    "1. Filter by county.\n",
    "2. Find row sums of votes. \n",
    "3. Sort by row sums. (there is only one case of a duplicate sum, in Black Hawk county)\n",
    "4. Profit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_counties = set()\n",
    "remaining_counties = set()\n",
    "\n",
    "election_name_df = pd.DataFrame(columns=[\"SoS Name\", \"VEST Name\", \"County\"])\n",
    "\n",
    "all_counties = vest_df[\"COUNTY\"].unique()\n",
    "\n",
    "# we will want a way to construct mappings from shapefile precincts\n",
    "# to VEST names and counties\n",
    "\n",
    "for county in all_counties:\n",
    "    v = vest_df[vest_df[\"COUNTY\"] == county]\n",
    "    s = sos_df[sos_df[\"County\"] == county]\n",
    "    \n",
    "    # Reset indexes\n",
    "    v.index = [i for i in range(len(v))]\n",
    "    s.index = [i for i in range(len(s))]\n",
    "    \n",
    "    # Save precinct names so we can convert for the merge\n",
    "    new_names = v[\"NAME\"]\n",
    "    old_names = s[\"Precinct\"]\n",
    "    \n",
    "    # Isolate vote counts in each dataframe\n",
    "    votes_v = v.iloc[:, 3:-1].dropna().astype('int')\n",
    "    votes_s = s.iloc[:, 2:-1].dropna().astype('int')\n",
    "    \n",
    "    \n",
    "    # Not foolproof, but used to sort rows, we hope that there\n",
    "    # are no duplicate row sums\n",
    "    votes_v[\"Row Sum\"] = votes_v.sum(axis=1)\n",
    "    votes_s[\"Row Sum\"] = votes_s.sum(axis=1)\n",
    "    \n",
    "    \n",
    "    votes_v.sort_values(by=\"Row Sum\", inplace=True)\n",
    "    votes_s.sort_values(by=\"Row Sum\", inplace=True)\n",
    "    \n",
    "    changed_new = new_names.iloc[votes_v.index]\n",
    "    changed_old = old_names.iloc[votes_s.index]\n",
    "\n",
    "    changed_new.index = [i for i in range(len(changed_new))]\n",
    "    changed_old.index = [i for i in range(len(changed_old))]\n",
    "    \n",
    "    votes_v.index = [i for i in range(len(votes_v))]\n",
    "    votes_s.index = [i for i in range(len(votes_s))]\n",
    "    \n",
    "    # If these two have the same value, we want to do the following work\n",
    "    if votes_s.equals(votes_v):\n",
    "        \n",
    "        temp = pd.DataFrame(data = {\"SoS Name\" : changed_old, \"VEST Name\" : changed_new})\n",
    "        temp[\"County\"] = county\n",
    "        election_name_df = pd.concat([election_name_df, temp], axis=0)\n",
    "        \n",
    "        validated_counties.add(county)\n",
    "        \n",
    "    else:\n",
    "        remaining_counties.add(county)\n",
    "        \n",
    "    \n",
    "election_name_df.to_csv(\"precinct_name_conversion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remaining_counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Black Hawk was the only county with a duplicate row sum that resulted in a difference in sorting rows. Will correct manually here to complete election_name_df for old to new precinct name conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black Hawk\n",
    "election_name_df = election_name_df[election_name_df[\"County\"] != \"Black Hawk\"]\n",
    "\n",
    "v = vest_df[vest_df[\"COUNTY\"] == \"Black Hawk\"]\n",
    "s = sos_df[sos_df[\"County\"] == \"Black Hawk\"]\n",
    "\n",
    "# Reset indexes\n",
    "v.index = [i for i in range(len(v))]\n",
    "s.index = [i for i in range(len(s))]\n",
    "\n",
    "\n",
    "# Isolate vote counts in each dataframe\n",
    "votes_v = v.iloc[:, 2:-1].dropna()\n",
    "votes_s = s.iloc[:, 1:-1].dropna()\n",
    "\n",
    "votes_v[\"Row Sum\"] = votes_v.sum(axis=1)\n",
    "votes_s[\"Row Sum\"] = votes_s.sum(axis=1)\n",
    "\n",
    "votes_v.sort_values(by=\"Row Sum\", inplace=True)\n",
    "votes_s.sort_values(by=\"Row Sum\", inplace=True)\n",
    "\n",
    "changed_new.index = [i for i in range(len(changed_new))]\n",
    "changed_old.index = [i for i in range(len(changed_old))]\n",
    "\n",
    "votes_v.index = [i for i in range(len(votes_v))]\n",
    "votes_s.index = [i for i in range(len(votes_s))]\n",
    "\n",
    "# There is a duplicate row sum, at index 46 and 47\n",
    "new_idxs = [i for i in range(len(votes_v))]\n",
    "new_idxs[46] = 47\n",
    "new_idxs[47] = 46\n",
    "\n",
    "votes_v = votes_v.iloc[new_idxs]\n",
    "votes_v.index = [i for i in range(len(votes_v))]\n",
    "\n",
    "changed_new = votes_v[\"NAME\"]\n",
    "changed_old = votes_s[\"Precinct\"]\n",
    "\n",
    "votes_v.drop(columns=[\"NAME\"], inplace=True)\n",
    "votes_s.drop(columns=[\"Precinct\"], inplace=True)\n",
    "\n",
    "\n",
    "assert(votes_v.astype('int').equals(votes_s.astype('int')))\n",
    "\n",
    "temp = pd.DataFrame(data = {\"SoS Name\" : changed_old, \"VEST Name\" : changed_new})\n",
    "temp[\"County\"] = \"Black Hawk\"\n",
    "election_name_df = pd.concat([election_name_df, temp], axis=0)\n",
    "election_name_df.to_csv(\"election_precinct_name_conversion.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join election results to dataframe with unique ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_name_df[\"ELECTION_MERGE_ID\"] = election_name_df[\"County\"] + \" \" + election_name_df[\"SoS Name\"]\n",
    "election_name_df[\"SHAPE_MERGE_ID\"] = election_name_df[\"County\"] + \" \" + election_name_df[\"VEST Name\"]\n",
    "\n",
    "sos_df[\"ELECTION_MERGE_ID\"] = sos_df[\"County\"] + \" \" + sos_df[\"Precinct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How many sos elections results don't have a precinct name to merge to?\")\n",
    "print(len(sos_df[~sos_df[\"ELECTION_MERGE_ID\"].isin(election_name_df[\"ELECTION_MERGE_ID\"])]))\n",
    "\n",
    "print(\"How many sos elections results don't have a precinct name to merge to?\")\n",
    "print(len(sos_df[~sos_df[\"ELECTION_MERGE_ID\"].isin(election_name_df[\"ELECTION_MERGE_ID\"])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(election_name_df.head())\n",
    "display(sos_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_election_results = election_name_df.merge(sos_df, on=\"ELECTION_MERGE_ID\", how=\"inner\")\n",
    "combined_election_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's look at shapefile data. This is going to be a bit more involved. \n",
    "\n",
    "#### First, validate all counties that weren't pulled from the county's data repo or the updated census bureau and had no special merges. \n",
    "\n",
    "#### Then, validate the 4 county shapefiles where VEST got the shapefile data directly from the county. \n",
    "\n",
    "#### Finally, validate the counties that were drawn using updated census data or had precinct merges performed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ultimately, we are unable to validate shapefile data for Polk, Scott, Dubuque, and Linn counties. That work can be found at the bottom of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(vest_df.head())\n",
    "display(shape_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1689, 18)\n",
      "(1677, 28)\n"
     ]
    }
   ],
   "source": [
    "# Shape_df has more 12 precincts than VEST, which is perfectly okay because those shape_df precincts \n",
    "# could have note received any votes. \n",
    "print(shape_df.shape)\n",
    "print(vest_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_only_prcs = vest_df[~vest_df[\"DISTRICT\"].isin(shape_df[\"DISTRICT\"])].copy()\n",
    "s_only_prcs = shape_df[~shape_df[\"DISTRICT\"].isin(vest_df[\"DISTRICT\"])].copy()\n",
    "\n",
    "print(\"Num districts in VEST not in shape:\", len(v_only_prcs))\n",
    "print(\"Num districts in shape not in VEST:\", len(s_only_prcs))\n",
    "\n",
    "# hmm so maybe a simple renaming is in order\n",
    "print(\"Prcs only in vest\")\n",
    "print('-' * 20)\n",
    "print(v_only_prcs[\"NAME\"].value_counts())\n",
    "print()\n",
    "\n",
    "print(\"Prcs only in shape\")\n",
    "print('-' * 20)\n",
    "print(s_only_prcs[\"NAME\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DISTRICT values within VEST and shape are not unique, with the lack of a county field in shape_df, there is no unique ID. \n",
    "#### As such, we will need to use the shapefiles themselves as a guess for which VEST precincts are assigned to what shape_df precincts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Does VEST have unique DISTRICT names?\", len(vest_df[\"DISTRICT\"]), len(vest_df[\"DISTRICT\"].unique()))\n",
    "print(\"Does shape have unique NAME names?\", len(shape_df[\"DISTRICT\"]), len(shape_df[\"DISTRICT\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The counties that will require special treatment, notice that Polk appears twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These counties had precincts merged by VEST\n",
    "merged_counties = {\"Appanoose\", \"Black Hawk\", \"Des Moines\", \"Fremont\", \"Lee\", \"Polk\", \"Tama\"}\n",
    "\n",
    "# These counties used shapefile data downloaded from county sources\n",
    "local_counties = {\"Dallas\", \"Johnson\", \"Linn\", \"Story\"}\n",
    "\n",
    "# These counties used shapefiles from the census \n",
    "census_counties = {\"Dubuque\", \"Marion\", \"Muscatine\", \"Polk\", \"Pottawattamie\", \"Scott\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These three functions will be the main workhorses for verifying the remaining shapefiles. \n",
    "\n",
    "##### **assign_names_by_geoms()** uses precinct geometries as unique IDs, getting a list of potential IDs to consider and seeing if they comprise the VEST precinct. \n",
    "\n",
    "##### **add_merge_column()** uses the found assignments to create a new column in the raw source data. \n",
    "\n",
    "##### **validate_assigned_geoms()** checks if the new precinct shapes (made by dissolving raw shapes based on assignments) have equal geometries to their mapped VEST precincts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_names_by_geoms(vest, raw, verbose=False, tolerance=3e-2, graph=False):\n",
    "    \"\"\"\n",
    "    For use in the case where one is trying to match shapefiles between two \n",
    "    GeoDataFrames but the geometries are the only real unique ID shared between\n",
    "    each file. \n",
    "    \n",
    "    @param vest (GeoDataFrame): VEST data for a single county\n",
    "    @param raw (GeoDataFrame): either census or locally downloaded shapefiles\n",
    "                                for a single county\n",
    "    @param verbose (bool): to print or not to print, that is the question\n",
    "    @param tolerance (float): ultimately, raw precinct shapefiles are considered to\n",
    "                                be outside of the VEST precinct shapefile if the difference\n",
    "                                in area after you subtract overlap between the raw and VEST shapefile \n",
    "                                is small. How small, you ask? That's up to ~tolerance~. In testing,\n",
    "                                if a raw precinct file falls outside of the VEST precinct, the area of \n",
    "                                the overlap is on the order of 1e-6. However, there are some cases where \n",
    "                                a raw precinct that is largely outside of VEST precinct will nibble away\n",
    "                                > .001 of the area. As such, the tolerance is set by default to .003. So,\n",
    "                                if the area overlap between a raw precinct and a VEST precinct is more than\n",
    "                                .3% of the VEST precinct's area, the raw precinct is mapped to the VEST precinct.\n",
    "                                    \n",
    "    \n",
    "    @return name2vest (dict): mapping of raw IDs to their corresponding NAME field\n",
    "                                in the vest dataframe. Used to dissolve shapefiles together. \n",
    "    \n",
    "    \"\"\"\n",
    "    # To be returned at the end, the bread and butter\n",
    "    name2vest = defaultdict(set)\n",
    "    invalid_geoms = set()\n",
    "    \n",
    "    \n",
    "    # This line looks weird, it's just grabbing each row from a \n",
    "    # geopandas GeoDataFrame without reverting the row to pandas, \n",
    "    # which happens by indexing via .iloc[]\n",
    "    rows = [vest[vest[\"NAME\"] == vest[\"NAME\"].iloc[i]] for i in range(len(vest))]\n",
    "\n",
    "    # aka for precinct in county\n",
    "    for row in rows:\n",
    "        vest_name = row[\"NAME\"].iloc[0]\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "            print(vest_name + \" Assign\")\n",
    "\n",
    "            print('-' * 25)\n",
    "        \n",
    "        # standardizing index for later comparison\n",
    "        row.index = [i for i in range(len(row))]\n",
    "        \n",
    "        # shapely object\n",
    "        geom = row[\"geometry\"].iloc[0]\n",
    "        \n",
    "        try:\n",
    "            # Get all geometries that are within or touch the shapely object\n",
    "            # of the current VEST precinct\n",
    "            relevant_prcs = raw[(raw[\"geometry\"].intersects(geom))]\n",
    "            contained_prcs = raw[(raw[\"geometry\"].within(geom))]\n",
    "            \n",
    "            # TODO: make this take precedence over the other precincts found\n",
    "            same_prcs = raw[(raw[\"geometry\"].geom_almost_equals(geom))]\n",
    "\n",
    "        except:\n",
    "            # Sometimes the geopandas binary predicates break, not quite sure why\n",
    "            print(f\"{vest_name} will need special attention, invalid geometries\")\n",
    "            invalid_geoms.add(vest_name)\n",
    "            \n",
    "        if verbose:\n",
    "                print(\"Num relevant prcs\", len(relevant_prcs))\n",
    "                print(\"Num contained prcs\", len(contained_prcs))\n",
    "                print(\"Num same_prcs prcs\", len(same_prcs))\n",
    "                print()\n",
    "        \n",
    "            \n",
    "        # to make sure things are working correctly, \n",
    "        # plot all precincts we've matched from shapefile\n",
    "        # that COULD be within the VEST precinct's shapefile\n",
    "        if graph:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(6, 6))\n",
    "            relevant_prcs.plot(ax=ax[0], color=\"Coral\")\n",
    "\n",
    "            # draw a red ring of the VEST precinct shape\n",
    "            # around the shape precincts we have selected\n",
    "            try:\n",
    "                ax[0].plot(*geom.exterior.xy, color=\"red\")\n",
    "            except AttributeError:\n",
    "                for g in geom.geoms:\n",
    "                    plt.plot(*g.exterior.xy, color=\"red\")\n",
    "        \n",
    "        \n",
    "        # the shape IDs that will be mapped to VEST name\n",
    "        keep_names = []\n",
    "        \n",
    "        names = relevant_prcs[\"NAME\"].to_list()\n",
    "        names.extend(contained_prcs[\"NAME\"].to_list())\n",
    "        names.extend(same_prcs[\"NAME\"].to_list())\n",
    "        \n",
    "        # it would be good if we could assign indices to shapefiles?\n",
    "        for name in list(set(names)):\n",
    "            \n",
    "            # again, preserving GeoDataFrame type\n",
    "            prc = relevant_prcs[relevant_prcs[\"NAME\"] == name].copy()\n",
    "            \n",
    "            orig_idx = tuple(prc.index.to_list())\n",
    "            \n",
    "            if graph:\n",
    "                prc.plot(ax=ax[0])\n",
    "            \n",
    "            # set index to 0\n",
    "            prc.index = [i for i in range(len(prc))]\n",
    "            \n",
    "            # see how much area the raw precinct and VEST precinct share\n",
    "            area_overlap = gp.overlay(row, prc, how=\"difference\")\n",
    "            \n",
    "            if graph:\n",
    "                ax[0].set_title(\"overlap\")\n",
    "                \n",
    "            area_diff = geom.area - area_overlap[\"geometry\"].area.sum()\n",
    "            percent_diff = area_diff / geom.area\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"{name} percent difference\", np.round(percent_diff, 4))\n",
    "            \n",
    "            # bug could crop up here, not sure about the change\n",
    "            if percent_diff > tolerance or name in contained_prcs[\"NAME\"].values:\n",
    "                keep_names.append((name, orig_idx))\n",
    "                \n",
    "        just_names = [key[0] for key in keep_names]\n",
    "        # Plot the raw precincts we have assignned, wrap VEST in red around\n",
    "        if graph:\n",
    "            relevant_prcs[relevant_prcs[\"NAME\"].isin(just_names)].plot(ax=ax[1])\n",
    "            \n",
    "        for name in keep_names:\n",
    "            name2vest[name].add(vest_name)\n",
    "        \n",
    "        if graph:\n",
    "            try:\n",
    "                ax[1].plot(*geom.exterior.xy, color=\"red\")\n",
    "            except AttributeError:\n",
    "                for g in geom.geoms:\n",
    "                    plt.plot(*g.exterior.xy, color=\"red\")\n",
    "\n",
    "            ax[1].set_title(\"Resulting assignment\")\n",
    "            fig.suptitle(vest_name)\n",
    "    \n",
    "        if verbose:\n",
    "            print()\n",
    "            print(\"Num prcs kept\", len(keep_names))\n",
    "\n",
    "    return name2vest, invalid_geoms\n",
    "\n",
    "\n",
    "def add_merge_column(raw, name2vest, verbose=False):\n",
    "    \"\"\"\n",
    "    Append a new column to raw, return the updated col?\n",
    "    \"\"\"\n",
    "    \n",
    "    multiple_assignments = set()\n",
    "    \n",
    "    for name, val in name2vest.items():\n",
    "        if len(val) > 1:\n",
    "            print(f\"{name} has multiple assignments:\", val)\n",
    "            # we may want to add something here, can we expand the size of raw?\n",
    "            # this could be promising, come back here\n",
    "            multiple_assignments.add(name)\n",
    "    \n",
    "    name_dict = {key[0] : val for key, val in name2vest.items()}\n",
    "    \n",
    "    # might be worth duplicating the rows for precincts that are assigned to multiple locations\n",
    "    raw[\"MERGE\"] = raw[\"NAME\"].apply(lambda name: list(name_dict[name])[0] if name in name_dict else name)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Raw dimensions before dissolve:\", raw.shape)\n",
    "        \n",
    "    # there could also be a bug here, merging combined too much\n",
    "    raw = raw.dissolve(by=\"MERGE\", aggfunc=\"first\").to_crs(CRS)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Raw dimensions after dissolve:\", raw.shape)\n",
    "        \n",
    "    raw.reset_index(inplace=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(raw[\"MERGE\"].value_counts())\n",
    "    print()\n",
    "    return raw, multiple_assignments\n",
    "\n",
    "\n",
    "def validate_assigned_geoms(vest, raw, invalid_geoms, verbose=False):\n",
    "    \"\"\"\n",
    "    After raw shapefiles have been assigned to VEST precincts\n",
    "    and merged, we can run geom_almost equals. For the precincts\n",
    "    that come up as unequal, print the difference in area, and if \n",
    "    that difference is larger than 0, plot the overlaid version of \n",
    "    both raw and precinct maps. \n",
    "    \n",
    "    @param vest (GeoDataFrame): VEST data for a single county\n",
    "    @param raw (GeoDataFrame): either census or locally downloaded shapefiles\n",
    "                                for a single county. Should have a \"MERGE\" column\n",
    "                                that maps to a vest \"NAME\" column.\n",
    "                                \n",
    "    @return no_corresponding (set): vest precincts that for some reason did not \n",
    "                                    have any raw precincts assigned to them\n",
    "    \"\"\"\n",
    "    labeled_incorrect = set()\n",
    "    geom_exception = set()\n",
    "    county = vest[\"COUNTY\"].value_counts().index.to_list()[0]\n",
    "    valid = True\n",
    "    for name in vest[\"NAME\"].value_counts().index:\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "            print(name + \" Validate\")\n",
    "            print('-' * 25)\n",
    "        \n",
    "        if name in invalid_geoms:\n",
    "            print(f\"Precinct {name} had invalid geometries, look at separately.\")\n",
    "            continue\n",
    "        \n",
    "        # Shapefiles that should be assigned to one another\n",
    "        v_row = vest[vest[\"NAME\"] == name]\n",
    "        # bug could be cropping up here, should check on raw merge\n",
    "        loc_row = raw[raw[\"MERGE\"] == name]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"VEST type\", type(v_row))\n",
    "            print(\"VEST dims\", v_row.shape)\n",
    "            \n",
    "            print(\"Raw type\", type(loc_row))\n",
    "            print(\"Raw dims\", loc_row.shape)\n",
    "        \n",
    "        \n",
    "        # See if the merged shapefiles are equal to VEST\n",
    "        try:\n",
    "            geom_equals = v_row[\"geometry\"].geom_almost_equals(loc_row[\"geometry\"], align=False)\n",
    "        except ValueError:\n",
    "            print(f\"Exception thrown for geom_almost_equals on {name}\")\n",
    "            geom_exception.add(name)\n",
    "            print()\n",
    "            continue\n",
    "            \n",
    "        # break down the precincts that didn't match \n",
    "        if not geom_equals.all():   \n",
    "            \n",
    "            v_area = v_row.geometry.area.to_numpy()\n",
    "            loc_area = loc_row.geometry.area.to_numpy()\n",
    "            difference = np.round(np.sum(np.abs(v_area - loc_area) / 1e6), 2)\n",
    "\n",
    "            if difference > 0.1:\n",
    "                valid = False\n",
    "                print(f\"Geoms reported unequal for {name}\")\n",
    "                print(f\"Difference in area is {difference} km^2\")\n",
    "                print()\n",
    "                \n",
    "                labeled_incorrect.add(name)\n",
    "                fig, ax = plt.subplots(1, 3, figsize=(12,4))\n",
    "                \n",
    "                type_v = str(type(v_row[\"geometry\"].iloc[0]))\n",
    "                type_loc = str(type(loc_row[\"geometry\"].iloc[0]))\n",
    "    \n",
    "                \n",
    "                if type_v != \"<class 'shapely.geometry.multipolygon.MultiPolygon'>\":              \n",
    "                    v_row.exterior.plot(ax=ax[0], color=\"Coral\")\n",
    "                    v_row.exterior.plot(ax=ax[2], color=\"Coral\")\n",
    "                    \n",
    "                else:\n",
    "                    for g in v_row[\"geometry\"].iloc[0].geoms:\n",
    "                        ax[0].plot(*g.exterior.xy, color=\"Coral\")\n",
    "                        ax[2].plot(*g.exterior.xy, color=\"Coral\")\n",
    "                        \n",
    "                \n",
    "                if type_loc != \"<class 'shapely.geometry.multipolygon.MultiPolygon'>\":\n",
    "                    loc_row.exterior.plot(ax=ax[1])\n",
    "                    loc_row.exterior.plot(ax=ax[2])\n",
    "                else:\n",
    "                    for g in loc_row[\"geometry\"].iloc[0].geoms:\n",
    "                        ax[1].plot(*g.exterior.xy, color=\"C0\")\n",
    "                        ax[2].plot(*g.exterior.xy, color=\"C0\")\n",
    "                    \n",
    "                \n",
    "                \n",
    "                ax[0].set_title(\"VEST\")\n",
    "                ax[1].set_title(county)\n",
    "                ax[2].set_title(\"Overlaid\")\n",
    "                fig.suptitle(name)\n",
    "    if valid:\n",
    "        print(f\"All shapefiles match for {county} county.\")\n",
    "    return (labeled_incorrect, geom_exception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the three functions above to validate all non-special counties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_counties = set()\n",
    "county2incorrect = {}\n",
    "problem_counties = set()\n",
    "\n",
    "# we will convert this to a geodataframe at the end, but may not need to, because\n",
    "# we will be joining to a geodataframe and will have to set the geometry\n",
    "# to be different\n",
    "shape_name_df = pd.DataFrame(columns=[\"VEST Name\", \"County\", \"geometry\"])\n",
    "\n",
    "for county in all_counties:\n",
    "    \n",
    "    if county in merged_counties or\\\n",
    "        county in census_counties or\\\n",
    "            county in local_counties:\n",
    "        continue\n",
    "        \n",
    "    print(county)\n",
    "    print('-' * 15)\n",
    "    \n",
    "    county_v = vest_df[vest_df[\"COUNTY\"] == county].copy()\n",
    "    county_sh = shape_df.copy()\n",
    "    \n",
    "    county2vest, invalid_county = assign_names_by_geoms(county_v, county_sh)\n",
    "    \n",
    "    # these are the idxs that contain the geoms that we want to match \n",
    "    # with the vest names and county\n",
    "    idxs = [key[1][0] for key in county2vest]\n",
    "    county_sh = county_sh.loc[idxs, :]\n",
    "    \n",
    "    county_sh, county_multiple = add_merge_column(county_sh, county2vest)\n",
    "    county_incorrect = validate_assigned_geoms(county_v, county_sh, invalid_county)\n",
    "    \n",
    "    \n",
    "    # unequal geoms and geom errors, respectively \n",
    "    if len(county_incorrect[0]) != 0 or len(county_incorrect[1]) != 0: \n",
    "        county2incorrect[county] = county_incorrect\n",
    "        problem_counties.add(county)\n",
    "    else:\n",
    "        validated_counties.add(county)\n",
    "    \n",
    "    # there is some issue going on here. Maybe an offsetting issue?\n",
    "    merge_data = pd.DataFrame(data = {\"VEST Name\" : county_sh[\"MERGE\"], \"geometry\" : county_sh[\"geometry\"]})\n",
    "    merge_data[\"County\"] = county\n",
    "    shape_name_df = pd.concat([shape_name_df, merge_data], axis=0)\n",
    "        \n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of counties validated first pass\", len(validated_counties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolve apparent errors in Fayette, Benton county. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_v = vest_df[vest_df[\"COUNTY\"] == \"Fayette\"].copy()\n",
    "temp_v = temp_v[temp_v[\"NAME\"] == \"Fairbank\"]\n",
    "temp_v.plot()\n",
    "\n",
    "fairbank_df = shape_df[shape_df[\"geometry\"].geom_almost_equals(temp_v[\"geometry\"].iloc[0])].copy()\n",
    "display(fairbank_df)\n",
    "fairbank_df.plot(figsize=(6,6))\n",
    "\n",
    "# add VEST name, county, and geometry\n",
    "temp = pd.DataFrame(data={\"County\" : [\"Fayette\"], \"VEST Name\" : [\"Fairbank\"], \n",
    "                    \"geometry\" : [fairbank_df[\"geometry\"].iloc[0]]})\n",
    "shape_name_df = pd.concat([shape_name_df, merge_data], axis=0, ignore_index=True)\n",
    "# county2vest_idxs[\"Fayette\"].append(fairbank_df.index.to_list()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter out Benton county and add it back in \n",
    "benton_df = vest_df[vest_df[\"COUNTY\"] == \"Benton\"]\n",
    "all_same = True\n",
    "for i in range(len(benton_df[\"geometry\"])):\n",
    "    geom_v = benton_df[\"geometry\"].iloc[i]\n",
    "    name_v = benton_df[\"NAME\"].iloc[i]\n",
    "    equal_shape = shape_df[shape_df[\"geometry\"].geom_almost_equals(geom_v)]\n",
    "    \n",
    "    geom_sh = equal_shape[\"geometry\"].iloc[0]\n",
    "    \n",
    "    \n",
    "    if len(equal_shape) != 1:\n",
    "        all_same = False\n",
    "        print(benton_df[\"NAME\"].iloc[i])\n",
    "        \n",
    "    else:\n",
    "        temp = pd.DataFrame(data={\"County\" : [\"Benton\"], \"geometry\" : [geom_sh], \"VEST Name\" : [name_v]})\n",
    "\n",
    "shape_name_df = pd.concat([shape_name_df, temp], axis=0, ignore_index=True)\n",
    "if all_same:\n",
    "    print(\"All shapefiles accurate for Benton county.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_counties.add(\"Webster\")\n",
    "validated_counties.add(\"Fayette\")\n",
    "validated_counties.add(\"Benton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Still have {set(all_counties).difference(validated_counties).difference(census_counties).difference(merged_counties).difference(local_counties)} to examine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will look at the counties that were taken from census data, contained merged precincts, and were downloaded from counties' local websites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapefiles that came from local county downloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dallas_df = gp.read_file(DATA_PATH + \"county_shapefiles/Dallas_data/VotingPrecincts.shp\").to_crs(CRS)\n",
    "johnson_df = gp.read_file(DATA_PATH + \"county_shapefiles/Johnson_data/Precincts_ALL_2013.shp\").to_crs(CRS)\n",
    "linn_df = gp.read_file(DATA_PATH + \"county_shapefiles/Linn_data/Voting_Precinct_Split.shp\").to_crs(CRS)\n",
    "story_df = gp.read_file(DATA_PATH + \"county_shapefiles/Story_data/StoryCoPrecincts20210326.shp\").to_crs(CRS)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "story_df.plot(ax=ax[0])\n",
    "johnson_df.plot(ax=ax[1])\n",
    "linn_df.plot(ax=ax[2])\n",
    "dallas_df.plot(ax=ax[3])\n",
    "\n",
    "ax[0].set_title(\"Story county\")\n",
    "ax[1].set_title(\"Johnson county\")\n",
    "ax[2].set_title(\"Linn county\")\n",
    "ax[3].set_title(\"Dallas county\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "johnson_v = vest_df[vest_df[\"COUNTY\"] == \"Johnson\"].copy()\n",
    "johnson_df = johnson_df.to_crs(CRS)\n",
    "johnson_df.rename(columns={\"PRECINCT\" : \"NAME\"}, inplace=True)\n",
    "\n",
    "johnson2vest, invalid_johnson = assign_names_by_geoms(johnson_v, johnson_df)\n",
    "johnson_df, johnson_multiple = add_merge_column(johnson_df, johnson2vest)\n",
    "johnson_incorrect = validate_assigned_geoms(johnson_v, johnson_df, invalid_johnson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add johnson data to merge df\n",
    "johnson_merge = pd.DataFrame(data={\"VEST Name\" : johnson_df[\"MERGE\"], \n",
    "                                   \"geometry\" : johnson_df[\"geometry\"]})\n",
    "johnson_merge[\"County\"] = \"Johnson\"\n",
    "shape_name_df = pd.concat([shape_name_df, johnson_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_v = vest_df[vest_df[\"COUNTY\"] == \"Story\"]\n",
    "story_df.rename(columns={\"PRECINCT\" : \"NAME\"}, inplace=True)\n",
    "\n",
    "story2vest, invalid_story = assign_names_by_geoms(story_v, story_df)\n",
    "story_df, story_multiple = add_merge_column(story_df, story2vest)\n",
    "story_incorrect = validate_assigned_geoms(story_v, story_df, invalid_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_merge = pd.DataFrame(data={\"VEST Name\" : story_df[\"MERGE\"], \n",
    "                                   \"geometry\" : story_df[\"geometry\"]})\n",
    "story_merge[\"County\"] = \"Story\"\n",
    "shape_name_df = pd.concat([shape_name_df, story_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dallas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dallas_v = vest_df[vest_df[\"COUNTY\"] == \"Dallas\"]\n",
    "dallas_df = dallas_df.to_crs(CRS)\n",
    "dallas_df.rename(columns={\"name\" : \"NAME\"}, inplace=True)\n",
    "\n",
    "dallas2vest, invalid_dallas = assign_names_by_geoms(dallas_v, dallas_df, tolerance=2e-2)\n",
    "dallas_df, dallas_multiple = add_merge_column(dallas_df, dallas2vest)\n",
    "dallas_incorrect = validate_assigned_geoms(dallas_v, dallas_df, invalid_dallas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dallas_merge = pd.DataFrame(data={\"VEST Name\" : dallas_df[\"MERGE\"], \n",
    "                                   \"geometry\" : dallas_df[\"geometry\"]})\n",
    "dallas_merge[\"County\"] = \"Dallas\"\n",
    "shape_name_df = pd.concat([shape_name_df, dallas_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adel 2 Precinct looks the most suspect, but the blue outline plotted is the shape of a single precinct in the locally downloaded file.\n",
    "\n",
    "##### The differences between De Soto, Van Meter, and Waukee 2 seem to align, with missing parts of each precinct appearing in another. VEST likely made these changes after contacting the county. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's verify the counties whose precinct shapes were taken from the census:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pottawattamie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pottawattamie_v = vest_df[vest_df[\"COUNTY\"] == \"Pottawattamie\"]\n",
    "pottawattamie_df = gp.read_file(\"raw-from-source/census_counties/pottawattamie/PVS_19_v2_vtd_19155.shp\").to_crs(CRS)\\\n",
    "\n",
    "pottawattamie_df.plot()\n",
    "pottawattamie_v.plot()\n",
    "\n",
    "pottawattamie2vest, pottawattamie_invalid = assign_names_by_geoms(pottawattamie_v, pottawattamie_df, tolerance=3e-2)\n",
    "pottawattamie_df, pottawattamie_incorrect = add_merge_column(pottawattamie_df, pottawattamie2vest)\n",
    "pottawattamie_incorrect = validate_assigned_geoms(pottawattamie_v, pottawattamie_df, pottawattamie_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pottawattamie_merge = pd.DataFrame(data={\"VEST Name\" : pottawattamie_df[\"MERGE\"], \n",
    "                                   \"geometry\" : pottawattamie_df[\"geometry\"]})\n",
    "pottawattamie_merge[\"County\"] = \"Pottawattamie\"\n",
    "shape_name_df = pd.concat([shape_name_df, pottawattamie_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Council Bluffs 4 looks the way it does because the raw data has a precinct that includes a few VEST precincts and a lake, \"Carter Lake\". This leads to some shape mismatching in VEST NAMEs Carter Lake 2, Carter Lake 1, Council Bluffs 1, and Council Bluffs 4. \n",
    "\n",
    "##### Otherwise, Pottawammie is good! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marion\n",
    "marion_v = vest_df[vest_df[\"COUNTY\"] == \"Marion\"]\n",
    "marion_df = gp.read_file(\"raw-from-source/census_counties/marion/PVS_19_v2_vtd_19125.shp\").to_crs(CRS)\n",
    "\n",
    "marion_df.plot()\n",
    "marion_v.plot()\n",
    "\n",
    "marion2vest, marion_invalid = assign_names_by_geoms(marion_v, marion_df)\n",
    "marion_df, marion_incorrect= add_merge_column(marion_df, marion2vest)\n",
    "marion_incorrect = validate_assigned_geoms(marion_v, marion_df, marion_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marion_v[marion_v[\"NAME\"].isin({'Knoxville 4', 'Knoxville 3', 'Knoxville 2', 'Knoxville 1', 'Knoxville'})].plot()\n",
    "marion_df[marion_df[\"NAME\"] == 'KNOXVILLE TWP'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marion_v[marion_v[\"NAME\"].isin({'Pella 3', 'Pella 2', 'Pella 1', 'Pella 4'})].plot()\n",
    "marion_df[marion_df[\"NAME\"] == 'PELLA IN MARION COUNTY W/LAKE PRAIRIE TWP PART'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marion_merge = pd.DataFrame(data={\"VEST Name\" : marion_df[\"MERGE\"], \n",
    "                                   \"geometry\" : marion_df[\"geometry\"]})\n",
    "marion_merge[\"County\"] = \"Marion\"\n",
    "shape_name_df = pd.concat([shape_name_df, marion_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VEST takes two large precincts from the raw file, \"PELLA IN MARION COUNTY W/LAKE PRAIRIE TWP PART\" and \"KNOXVILLE TWP\" and split that into 4 and 5 counties, respectively. Otherwise, things are good! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muscatine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muscatine\n",
    "muscatine_v = vest_df[vest_df[\"COUNTY\"] == \"Muscatine\"]\n",
    "muscatine_df = gp.read_file(\"raw-from-source/census_counties/muscatine/PVS_19_v2_vtd_19139.shp\").to_crs(CRS)\n",
    "\n",
    "muscatine_df.plot()\n",
    "muscatine_v.plot()\n",
    "\n",
    "muscatine2vest, muscatine_invalid = assign_names_by_geoms(muscatine_v, muscatine_df)\n",
    "muscatine2vest[('WAPSINONOC TWP', (0,))] = {'Muscatine County West Liberty 1/ Wapsi Prec'}\n",
    "\n",
    "muscatine_df, muscatine_incorrect = add_merge_column(muscatine_df, muscatine2vest)\n",
    "muscatine_incorrect = validate_assigned_geoms(muscatine_v, muscatine_df, muscatine_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(muscatine_incorrect[1])\n",
    "west_liberty_2 = muscatine_v[muscatine_v[\"NAME\"] == next(iter(muscatine_incorrect[1]))]\n",
    "west_liberty_2_sh = muscatine_df[muscatine_df[\"geometry\"].intersects(west_liberty_2[\"geometry\"].iloc[0])]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12,4))\n",
    "west_liberty_2.plot(ax=ax[0])\n",
    "west_liberty_2_sh.plot(ax=ax[1], cmap=\"Pastel1\")\n",
    "west_liberty_2_sh.plot(ax=ax[2], cmap=\"Pastel1\")\n",
    "west_liberty_2.plot(ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### West Liberty is a precinct that VEST created, doesn't appear to be present in the census shapefile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muscatine_df\n",
    "muscatine_merge = pd.DataFrame(data={\"VEST Name\" : muscatine_df[\"MERGE\"], \n",
    "                                   \"geometry\" : muscatine_df[\"geometry\"]})\n",
    "muscatine_merge[\"County\"] = \"Muscatine\"\n",
    "shape_name_df = pd.concat([shape_name_df, muscatine_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's do the counties that contained precincts that VEST merged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_unassigned(vest, shape, unassigned):\n",
    "    \"\"\"\n",
    "    Used to visualize why unassigned VEST precincts did not\n",
    "    have any corresponding raw shapefiles. \n",
    "    \"\"\"\n",
    "    for prc in unassigned:\n",
    "        curr_v = vest[vest[\"NAME\"] == prc].copy()  \n",
    "        geom = curr_v[\"geometry\"].iloc[0]\n",
    "        \n",
    "        intersects = shape[shape[\"geometry\"].intersects(geom)].copy()\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        intersects.plot(ax=ax, cmap=\"Pastel1\")\n",
    "        try: \n",
    "            ax.plot(*geom.exterior.xy)\n",
    "        except:\n",
    "            for g in geom.geoms:\n",
    "                ax.plot(*g.exterior.xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tama_v = vest_df[vest_df[\"COUNTY\"] == \"Tama\"].copy()\n",
    "tama_df = shape_df.copy()\n",
    "\n",
    "tama2vest, tama_invalid = assign_names_by_geoms(tama_v, tama_df)\n",
    "idxs = [key[1][0] for key in tama2vest]\n",
    "tama_df = tama_df.loc[idxs, :]\n",
    "\n",
    "tama_df, _ = add_merge_column(tama_df, tama2vest)\n",
    "tama_incorrect = validate_assigned_geoms(tama_v, tama_df, tama_invalid)\n",
    "\n",
    "display_unassigned(tama_v, tama_df, tama_incorrect[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tama_merge = pd.DataFrame(data={\"VEST Name\" : tama_df[\"MERGE\"], \n",
    "                                   \"geometry\" : tama_df[\"geometry\"]})\n",
    "tama_merge[\"County\"] = \"Tama\"\n",
    "shape_name_df = pd.concat([shape_name_df, tama_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The only precinct that is off is VEST \"Tama 14 - Toledo City\", which when added to precinct \"Tama 13 - Howard/Toledo 1/Toledo Twp\", matched exactly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Black Hawk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_hawk_v = vest_df[vest_df[\"COUNTY\"] == \"Black Hawk\"].copy()\n",
    "black_hawk_df = shape_df.copy()\n",
    "\n",
    "black_hawk2vest, black_hawk_invalid = assign_names_by_geoms(black_hawk_v, black_hawk_df)\n",
    "\n",
    "idxs = [key[1][0] for key in black_hawk2vest]\n",
    "black_hawk_df = black_hawk_df.loc[idxs, :]\n",
    "\n",
    "black_hawk_df, _ = add_merge_column(black_hawk_df, black_hawk2vest)\n",
    "black_hawk_incorrect = validate_assigned_geoms(black_hawk_v, black_hawk_df, black_hawk_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_hawk_merge = pd.DataFrame(data={\"VEST Name\" : black_hawk_df[\"MERGE\"], \n",
    "                                   \"geometry\" : black_hawk_df[\"geometry\"]})\n",
    "black_hawk_merge[\"County\"] = \"Black Hawk\"\n",
    "shape_name_df = pd.concat([shape_name_df, black_hawk_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Des Moines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Des moines\n",
    "des_moines_v = vest_df[vest_df[\"COUNTY\"] == \"Des Moines\"].copy()\n",
    "des_moines_df = shape_df.copy()\n",
    "\n",
    "des_moines2vest, des_moines_invalid = assign_names_by_geoms(des_moines_v, des_moines_df)\n",
    "idxs = [key[1][0] for key in des_moines2vest]\n",
    "des_moines_df = des_moines_df.loc[idxs, :]\n",
    "des_moines_df, _ = add_merge_column(des_moines_df, des_moines2vest)\n",
    "des_moines_incorrect = validate_assigned_geoms(des_moines_v, des_moines_df, des_moines_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_moines_merge = pd.DataFrame(data={\"VEST Name\" : des_moines_df[\"MERGE\"], \n",
    "                                   \"geometry\" : des_moines_df[\"geometry\"]})\n",
    "des_moines_merge[\"County\"] = \"Des Moines\"\n",
    "shape_name_df = pd.concat([shape_name_df, des_moines_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fremont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fremont\n",
    "fremont_v = vest_df[vest_df[\"COUNTY\"] == \"Fremont\"].copy()\n",
    "fremont_outline = fremont_v.dissolve(by=\"COUNTY\")[\"geometry\"].iloc[0]\n",
    "fremont_df = shape_df[(shape_df[\"geometry\"].intersects(fremont_outline)) | (shape_df[\"geometry\"].covered_by(fremont_outline))].copy()\n",
    "\n",
    "fremont2vest, fremont_invalid = assign_names_by_geoms(fremont_v, fremont_df)\n",
    "\n",
    "idxs = [key[1][0] for key in fremont2vest]\n",
    "fremont_df = fremont_df.loc[idxs, :]\n",
    "\n",
    "fremont_df, _ = add_merge_column(fremont_df, fremont2vest)\n",
    "fremont_incorrect = validate_assigned_geoms(fremont_v, fremont_df, fremont_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fremont_merge = pd.DataFrame(data={\"VEST Name\" : fremont_df[\"MERGE\"], \n",
    "                                   \"geometry\" : fremont_df[\"geometry\"]})\n",
    "fremont_merge[\"County\"] = \"Fremont\"\n",
    "shape_name_df = pd.concat([shape_name_df, fremont_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee\n",
    "lee_v = vest_df[vest_df[\"COUNTY\"] == \"Lee\"]\n",
    "lee_df = shape_df.copy()\n",
    "\n",
    "lee2vest, lee_invalid = assign_names_by_geoms(lee_v, lee_df)\n",
    "idxs = [key[1][0] for key in lee2vest]\n",
    "lee_df = lee_df.loc[idxs, :]\n",
    "lee_df, _ = add_merge_column(lee_df, lee2vest)\n",
    "lee_incorrect = validate_assigned_geoms(lee_v, lee_df, lee_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lee_merge = pd.DataFrame(data={\"VEST Name\" : lee_df[\"MERGE\"], \n",
    "                                   \"geometry\" : lee_df[\"geometry\"]})\n",
    "lee_merge[\"County\"] = \"Lee\"\n",
    "shape_name_df = pd.concat([shape_name_df, lee_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appanoose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appanoose\n",
    "appanoose_v = vest_df[vest_df[\"COUNTY\"] == \"Appanoose\"]\n",
    "appanoose_df = shape_df.copy()\n",
    "\n",
    "appanoose2vest, appanoose_invalid = assign_names_by_geoms(appanoose_v, appanoose_df)\n",
    "idxs = [key[1][0] for key in appanoose2vest]\n",
    "appanoose_df = appanoose_df.loc[idxs, :]\n",
    "appanoose_df, _ = add_merge_column(appanoose_df, appanoose2vest)\n",
    "appanoose_incorrect = validate_assigned_geoms(appanoose_v, appanoose_df, appanoose_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appanoose_merge = pd.DataFrame(data={\"VEST Name\" : appanoose_df[\"MERGE\"], \n",
    "                                   \"geometry\" : appanoose_df[\"geometry\"]})\n",
    "appanoose_merge[\"County\"] = \"Appanoose\"\n",
    "shape_name_df = pd.concat([shape_name_df, appanoose_merge], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_name_df[\"SHAPE_MERGE_ID\"] = shape_name_df[\"County\"] + \" \" + shape_name_df[\"VEST Name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(shape_name_df[~shape_name_df[\"SHAPE_MERGE_ID\"].isin(combined_election_results[\"SHAPE_MERGE_ID\"])]))\n",
    "\n",
    "missing = shape_name_df[~shape_name_df[\"SHAPE_MERGE_ID\"].isin(combined_election_results[\"SHAPE_MERGE_ID\"])]\n",
    "missing_v = combined_election_results[~combined_election_results[\"SHAPE_MERGE_ID\"].isin(shape_name_df[\"SHAPE_MERGE_ID\"])]\n",
    "\n",
    "print(missing[\"County\"].value_counts())\n",
    "print(missing_v[\"County_x\"].value_counts())\n",
    "print(len(shape_name_df))\n",
    "print(len(election_name_df))\n",
    "print(len(vest_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_election_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreated_vest = combined_election_results.merge(shape_name_df, how=\"left\", on=\"SHAPE_MERGE_ID\")\n",
    "recreated_vest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vest_df[\"VEST_MERGE\"] = vest_df[\"COUNTY\"] + \" \" + vest_df[\"NAME\"]\n",
    "recreated_vest[\"VEST_MERGE\"] = recreated_vest[\"County_x\"] + \" \" + recreated_vest[\"VEST Name_x\"]\n",
    "\n",
    "print(\"Do we have any VEST IDs not present in recreated VEST?\")\n",
    "print(len(recreated_vest[~recreated_vest[\"VEST_MERGE\"].isin(vest_df[\"VEST_MERGE\"])]))\n",
    "print()\n",
    "\n",
    "print(\"Do we have any VEST IDs not present in VEST?\")\n",
    "print(len(vest_df[~vest_df[\"VEST_MERGE\"].isin(recreated_vest[\"VEST_MERGE\"])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df = vest_df.merge(recreated_vest, on=\"VEST_MERGE\", how=\"outer\")\n",
    "validate_df.head()\n",
    "columns=['G18GOVRREY', 'G18GOVDHUB', 'G18GOVLPOR', 'G18GOVOSIE', 'G18GOVOWRI', \n",
    "         'G18ATGDMIL', 'G18ATGLBAT', 'G18ATGOWRI', 'G18SOSRPAT', 'G18SOSDDEJ', \n",
    "         'G18SOSLOFE', 'G18SOSOWRI', 'G18TRERDAV', 'G18TREDFIT', 'G18TRELHIR', \n",
    "         'G18TREOWRI', 'G18AUDRMOS', 'G18AUDDSAN', 'G18AUDLPER', 'G18AUDOWRI', \n",
    "         'G18AGRRNAI', 'G18AGRDGAN', 'G18AGRLSTE', 'G18AGROWRI']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary validation results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, validate election results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are election results the same? True\n",
      "1686\n"
     ]
    }
   ],
   "source": [
    "# Now we can begin some validation on real values, this is exciting. \n",
    "# Let's try to verify on a county wide basis \n",
    "def validater_row_vect(df, name_column, column_list, verbose=False):\n",
    "    matching_rows = 0\n",
    "    different_rows = 0\n",
    "    \n",
    "    county_join_cols = (df[name_column]).to_numpy()\n",
    "    \n",
    "    # it's because before, we were checking that a row was entirely consistent\n",
    "    # here, we are double checking a lot\n",
    "    # so, let's just keep one track of the rows that are messed up\n",
    "    \n",
    "    wrong_idxs = np.zeros(len(df))\n",
    "    for i in column_list:\n",
    "        left_data = df[i + \"_x\"].to_numpy()\n",
    "        right_data = df[i + \"_y\"].to_numpy()\n",
    "        \n",
    "        local_idxs = np.where(left_data != right_data)\n",
    "        wrong_idxs[local_idxs] = 1\n",
    "        #print(\"Wrong idxs\", wrong_idxs)\n",
    "        \n",
    "    # we are close, we get the same result, but are double adding lots of rows\n",
    "    different_rows += np.sum(wrong_idxs)\n",
    "    matching_rows += len(df) - different_rows\n",
    "    \n",
    "    diff_list = county_join_cols[np.where(wrong_idxs == 1)]\n",
    "    diff_counties = list(set([county[:2] for county in diff_list]))\n",
    "    \n",
    "    if int(different_rows) != 0 or verbose:\n",
    "        print(\"There are \", len(df.index),\" total rows\")\n",
    "        print(f\"{int(different_rows)} of these rows have election result differences\")\n",
    "        print(f\"{int(matching_rows)} of these rows are the same\")\n",
    "        print(diff_list)\n",
    "        print(\"\")\n",
    "        \n",
    "    return (int(different_rows) == 0, diff_list)\n",
    "\n",
    "print(\"Are election results the same?\", validater_row_vect(validate_df, \"VEST_MERGE\", columns)[0])\n",
    "print(len(validate_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt to validate shapefile results, we are expecting some differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vest_geoms = gp.GeoSeries(validate_df[\"geometry_x\"], crs=CRS)\n",
    "recreated_geoms = gp.GeoSeries(validate_df[\"geometry_y\"], crs=CRS)\n",
    "\n",
    "area_vest = vest_geoms.area\n",
    "area_recreated = recreated_geoms.area\n",
    "\n",
    "equal_idxs = vest_geoms.geom_almost_equals(recreated_geoms, align=False)\n",
    "print(equal_idxs.value_counts())\n",
    "\n",
    "area_vest = area_vest[~equal_idxs]\n",
    "area_recreated = area_recreated[~equal_idxs]\n",
    "\n",
    "print(\"The average different in area between reportedly mis-matched precincts:\", np.round(np.mean(np.abs(area_vest - area_recreated) / 1e6),2))\n",
    "\n",
    "# for which counties do we have errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While it may have been possible to validate some precinct shapefiles in Polk, Scott, Dubuque, and Linn county, we intentionally leave all out for the sake of simplicity. Let's filter them out of the incorrect precinct shapes now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Which counties have errors?\")\n",
    "\n",
    "incorrect_validate_shapes = validate_df[~equal_idxs].copy()\n",
    "incorrect_validate_shapes = incorrect_validate_shapes[~incorrect_validate_shapes[\"COUNTY\"].isin([\"Polk\", \"Scott\", \"Linn\", \"Dubuque\"])]\n",
    "print(incorrect_validate_shapes[\"COUNTY\"][~equal_idxs].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_area_vest = gp.GeoSeries(incorrect_validate_shapes[\"geometry_x\"], crs=CRS).area.to_numpy()\n",
    "incorrect_area_recreated = gp.GeoSeries(incorrect_validate_shapes[\"geometry_y\"], crs=CRS).area.to_numpy()\n",
    "\n",
    "area_difference = np.abs(incorrect_area_vest - incorrect_area_recreated) / 1e6\n",
    "\n",
    "print(f\"{np.round(len(area_difference[area_difference < .1]) / len(area_difference) * 100, 2)} percent of precincts have a difference in area of less than .1 km^2\")\n",
    "print(f\"{np.round(len(area_difference[area_difference < 1]) / len(area_difference) * 100, 2)} percent of precincts have a difference in area of less than 1 km^2\")\n",
    "\n",
    "print(\"The precincts for which differences are larger have been displayed and highlighted previously in this notebook.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precincts and counties for which the difference in shapefile is larger than 1 km^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benton: Belle Plaine 1 3.07km^2\n",
      "Dallas: Adel 2 Precinct 2.22km^2\n",
      "Dallas: Van Meter 2.6km^2\n",
      "Fayette: Fairbank 172.57km^2\n",
      "Marion: Clay 3.11km^2\n",
      "Marion: Knoxville 1 500.24km^2\n",
      "Marion: Pella 4 29.22km^2\n",
      "Muscatine: Muscatine County West Liberty 1/ Wapsi Prec 4.43km^2\n",
      "Pottawattamie: Avoca 4.04km^2\n",
      "Pottawattamie: Carter Lake 1 10.6km^2\n",
      "Pottawattamie: Council Bluffs 4 1.38km^2\n",
      "Pottawattamie: Neola 3.08km^2\n",
      "Pottawattamie: Oakland 9.73km^2\n",
      "Tama: Tama 13 - Howard/Toledo 2.29km^2\n",
      "Tama: Tama 14 - Toledo City 2.29km^2\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = False\n",
    "incorrect_validate_shapes[\"VEST area\"] = incorrect_area_vest\n",
    "incorrect_validate_shapes[\"Shape area\"] = incorrect_area_recreated\n",
    "\n",
    "incorrect_validate_shapes[\"Area diff\"] = np.round(area_difference,2)\n",
    "\n",
    "area_diff= incorrect_validate_shapes[incorrect_validate_shapes[\"Area diff\"] > 1].copy()\n",
    "\n",
    "incorrect = []\n",
    "for vest_name, county, area in zip(area_diff[\"NAME\"], area_diff[\"COUNTY\"], area_diff[\"Area diff\"]):\n",
    "    incorrect.append(county + \": \" + vest_name + \" \" + str(area) + \"km^2\")\n",
    "    \n",
    "for prc in sorted(incorrect):\n",
    "    print(prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the work to attempt to validate shapefile data for Linn, Dubuque, Polk, and Scott county. This work is a bit less clean, but will hopefully illustrate that recreating VEST's work for these counties was not as straightforward, and why we were ultimately unable to validate VEST's work. \n",
    "\n",
    "#### While there are precincts within each county's raw shapefile that do largely agree with VEST, the alterations of a sufficient number of precincts are enough for us to report the county as separate from the list of validated counties. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linn_v = vest_df[vest_df[\"COUNTY\"] == \"Linn\"].copy()\n",
    "linn_df = linn_df.to_crs(CRS)\n",
    "linn_df.rename(columns={\"PCT_LG_NAM\" : \"NAME\"}, inplace=True)\n",
    "\n",
    "unmatched_names_df = linn_df[~linn_df[\"NAME\"].isin(linn_v[\"NAME\"])][\"NAME\"]\n",
    "unmatched_names_v = linn_v[~linn_v[\"NAME\"].isin(linn_df[\"NAME\"])][\"NAME\"]\n",
    "\n",
    "name_assignment={}\n",
    "for a, b in zip(sorted(unmatched_names_df.to_list()), sorted([val for val in unmatched_names_v.to_list() if val != \"Fairfax Township\"])):\n",
    "    name_assignment[b] = a\n",
    "\n",
    "linn_df[\"NAME\"] = linn_df[\"NAME\"].apply(lambda x: name_assignment[x] if x in name_assignment else x)\n",
    "same_linn_v = linn_v[linn_v[\"NAME\"].isin(linn_df[\"NAME\"])].copy()\n",
    "\n",
    "for i in range(len(linn_v)):\n",
    "    geom = linn_v[\"geometry\"].iloc[i]\n",
    "    name = linn_v[\"NAME\"].iloc[i]\n",
    "    \n",
    "    shape_prcs = linn_df[linn_df[\"NAME\"] == name].copy()\n",
    "    \n",
    "    v_area = np.array(geom.area)\n",
    "    sh_area = shape_prcs[\"geometry\"].area.to_numpy()\n",
    "    \n",
    "    area_diff = np.sum(np.abs(v_area - sh_area)) / 1e6\n",
    "    \n",
    "    colors=[\"green\", \"orange\", \"red\", \"blue\", \"purple\"]\n",
    "    if area_diff > 1e-4:\n",
    "        fig, ax = plt.subplots(figsize=(6,6))\n",
    "        shape_prcs.plot(ax=ax)\n",
    "        try:\n",
    "            ax.plot(*shape_prcs[\"geometry\"].iloc[0].exterior.xy, color=colors[0])\n",
    "        except AttributeError:\n",
    "            for idx, g in enumerate(shape_prcs[\"geometry\"].iloc[0].geoms):\n",
    "                ax.plot(*g.exterior.xy, color=colors[idx])\n",
    "                \n",
    "        linn_v[linn_v[\"NAME\"] == name].plot(ax=ax, cmap=\"Pastel1\")       \n",
    "        ax.set_title(name)\n",
    "        print(f\"Difference in area between {name}: {np.round(area_diff,2)} km^2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scott\n",
    "scott_v = vest_df[vest_df[\"COUNTY\"] == \"Scott\"]\n",
    "scott_df = gp.read_file(\"raw-from-source/census_counties/scott/PVS_19_v2_vtd_19163.shp\").to_crs(CRS)\n",
    "\n",
    "scott_df.plot()\n",
    "scott_v.plot()\n",
    "\n",
    "scott2vest, scott_invalid = assign_names_by_geoms(scott_v, scott_df)\n",
    "scott2vest[('ALLENS GROVE TWP W/O DIXON', (1,))] = {\"Allen's Grove\"}\n",
    "scott2vest[('BUTLER TWP', (8,))] = {\"Parkview\"}\n",
    "scott2vest[('ELDRIDGE W/SHERIDAN TWP PART', (14,))] = {\"Eldridge 2\"}\n",
    "scott2vest[('LE CLAIRE W/LE CLAIRE TWP PART', (16,))] = {\"LeClaire 1\"}\n",
    "\n",
    "scott_df, scott_incorrect = add_merge_column(scott_df, scott2vest)\n",
    "scott_incorrect = validate_assigned_geoms(scott_v, scott_df, scott_invalid)\n",
    "\n",
    "for prc in scott_incorrect[1]:\n",
    "    curr_v = scott_v[scott_v[\"NAME\"] == prc].copy()\n",
    "    geom = curr_v[\"geometry\"].iloc[0]\n",
    "    curr_sh = scott_df[scott_df[\"geometry\"].intersects(geom)]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    curr_sh.plot(ax=ax, cmap=\"Pastel1\")\n",
    "    ax.plot(*geom.exterior.xy)\n",
    "    ax.set_title(prc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polk\n",
    "polk_v = vest_df[vest_df[\"COUNTY\"] == \"Polk\"].copy()\n",
    "polk_df = gp.read_file(\"raw-from-source/census_counties/polk/PVS_19_v2_vtd_19153.shp\").to_crs(CRS)\n",
    "\n",
    "polk_df.plot()\n",
    "polk_v.plot()\n",
    "\n",
    "polk2vest, polk_invalid = assign_names_by_geoms(polk_v, polk_df)\n",
    "idxs = [key[1][0] for key in polk2vest]\n",
    "polk_df = polk_df.loc[idxs, :]\n",
    "\n",
    "polk2vest[('POLK CITY', (14,))] = {'Polk County Polk City'}\n",
    "polk2vest[('WEBSTER PCT 1', (23,))] = {'Polk County Webster 1'}\n",
    "polk2vest[('ANKENY PCT 12', (164,))] = {'Ankeny Precinct 12'}\n",
    "polk2vest[('DES MOINES PCT 71', (103,))] = {'Des Moines Precinct 71'}\n",
    "polk2vest[('FOUR MILE PCT 1', (9,))] = {'Polk County Four Mile 1'}\n",
    "polk2vest[('DES MOINES PCT 1', (39,))] = {'Des Moines Precinct 1'}\n",
    "polk2vest[('DES MOINES PCT 29', (63,))] = {'Des Moines Precinct 29'}\n",
    "polk2vest[('DES MOINES PCT 5', (48,))] = {'Polk County Saylor 2/ Des Moines Prct 5A'}\n",
    "polk2vest[('ALLEN PCT 1 W/O CARLISLE', (35,))] = {'Polk County Allen 1'}\n",
    "polk2vest[('DELAWARE PCT 1', (15,))] = {'Polk County Delaware 1'}\n",
    "polk2vest[('SAYLOR PCT 1', (26,))] = {'Polk County Saylor 1'}\n",
    "polk2vest[('URBANDALE PCT 4', (141,))] = {'Urbandale Precinct 4'}\n",
    "polk2vest[('DES MOINES PCT 64', (77,))] = {'Des Moines Precinct 64'}\n",
    "polk2vest[('JOHNSTON PCT 5', (148,))] = {'Des Moines Precinct 64'}\n",
    "polk2vest[('GRIMES W/JEFFERSON TWP & WEBSTER TWP & URBANDALE PARTS', (21,))] = \\\n",
    "    polk2vest[('GRIMES W/JEFFERSON TWP & WEBSTER TWP & URBANDALE PARTS', (21,))].difference({\"Grimes Precinct 3\"})\n",
    "\n",
    "polk_df, polk_incorrect = add_merge_column(polk_df, polk2vest)\n",
    "polk_incorrect = validate_assigned_geoms(polk_v, polk_df, polk_invalid)\n",
    "\n",
    "print(len(polk_incorrect[1]))\n",
    "for prc in polk_incorrect[1]:\n",
    "    curr_v = polk_v[polk_v[\"NAME\"] == prc].copy()\n",
    "    geom = curr_v[\"geometry\"].iloc[0]\n",
    "    curr_sh = polk_df[polk_df[\"geometry\"].intersects(geom)]\n",
    "    fig, ax = plt.subplots()\n",
    "    curr_sh.plot(ax=ax, cmap=\"Pastel1\")\n",
    "    ax.plot(*geom.exterior.xy, color=\"limegreen\")\n",
    "    ax.set_title(f\"VEST prc {prc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dubuque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dubuque_v = vest_df[vest_df[\"COUNTY\"] == \"Dubuque\"]\n",
    "dubuque_df = gp.read_file(\"raw-from-source/census_counties/dubuque/PVS_19_v2_vtd_19061.shp\").to_crs(CRS)\n",
    "\n",
    "# dubuque_df.plot()\n",
    "# dubuque_v.plot()\n",
    "\n",
    "print(dubuque_v.shape)\n",
    "print(dubuque_df.shape)\n",
    "\n",
    "print(sorted(dubuque_df[\"NAME\"].value_counts().index.to_list()))\n",
    "print(sorted(dubuque_v[\"NAME\"].value_counts().index.to_list()))\n",
    "\n",
    "# Dubuque\n",
    "dubuque_v = vest_df[vest_df[\"COUNTY\"] == \"Dubuque\"]\n",
    "dubuque_df = gp.read_file(\"raw-from-source/census_counties/dubuque/PVS_19_v2_vtd_19061.shp\").to_crs(CRS)\n",
    "\n",
    "dubuque_df.plot()\n",
    "dubuque_v.plot()\n",
    "\n",
    "dubuque2vest, dubuque_invalid = assign_names_by_geoms(dubuque_v, dubuque_df)\n",
    "\n",
    "# need to correct some things by hand\n",
    "dubuque2vest[('DUBUQUE PCT 15', (36,))] = {'Dubuque 12'}\n",
    "dubuque2vest[('TABLE MOUND TWP PART', (25,))] = {'Table Mound West - Dubuque precinct 45'}\n",
    "dubuque2vest[('TAYLOR TWP W/O FARLEY', (17,))] = {'Iowa/Taylor - Dubuque precinct 41'}\n",
    "dubuque2vest[('ASBURY', (5,))] = {'Asbury East - Dubuque precinct 31'}\n",
    "dubuque2vest[('DYERSVILLE IN DUBUQUE COUNTY', (8,))] = {'Dyersville/Dodge - Dubuque precinct 39'}\n",
    "dubuque2vest[('CENTER TWP PART W/O ASBURY & CENTRALIA', (4,))] = {'Center South/Vernon - Dubuque 34'}\n",
    "dubuque2vest[('DODGE TWP W/O DYERSVILLE & FARLEY & WORTHINGTON', (7,))] = {'Dyersville/Dodge - Dubuque precinct 39'}\n",
    "dubuque2vest[('CASCADE IN DUBUQUE COUNTY', (1,))] = {'Cascade/White Water - Dubuque 33'}\n",
    "dubuque2vest[('RICKARDSVILLE', (31,))] = {'Jefferson/Peru - Dubuque 42'}\n",
    "dubuque2vest[('WORTHINGTON', (32,))] = {'Dyersville/Dodge - Dubuque precinct 39'}\n",
    "dubuque2vest[('CENTRALIA', (1,))] = {'Center South/Vernon - Dubuque 34'}\n",
    "dubuque2vest[('ZWINGLE IN DUBUQUE COUNTY', (32,))] = {'Mosalem/Table Mound E/Wshngtn - Dubuque 43'}\n",
    "\n",
    "dubuque_df, dubuque_incorrect = add_merge_column(dubuque_df, dubuque2vest)\n",
    "dubuque_incorrect = validate_assigned_geoms(dubuque_v, dubuque_df, dubuque_invalid)\n",
    "\n",
    "assigned_dubuque = {key[0] for key in dubuque2vest}\n",
    "left_out = [prc for prc in dubuque_df[\"NAME\"] if prc not in assigned_dubuque]\n",
    "\n",
    "for name in left_out:\n",
    "    curr = dubuque_df[dubuque_df[\"NAME\"] == name].copy()\n",
    "    geom = curr[\"geometry\"].iloc[0]\n",
    "    display(curr.head())\n",
    "    fig, ax = plt.subplots()\n",
    "    intersects = dubuque_df[dubuque_df[\"geometry\"].intersects(geom)]\n",
    "    intersects.plot(ax=ax, cmap=\"Pastel1\")\n",
    "    ax.plot(*geom.exterior.xy, color=\"limegreen\")\n",
    "    ax.set_title(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
